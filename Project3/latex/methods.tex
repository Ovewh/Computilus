\section{Methods}

The integral we will solve is \cref{eq:int}, where r$_i$ is the distance from
origo given by \cref{eq:distance}. The integral has an analytical solution of
$5\pi^2/16^2$, so we will be able to calculate errors.

\begin{equation}
  \label{eq:int}
  I = \int_{-\infty}^{\infty} d\*r_1 d\*r_2 e^{-4(r_1+r_2)}
  \frac{1}{\abs{\*r_1 - \*r_2}}
\end{equation}

\begin{equation}
  \label{eq:distance}
r_i = \sqrt{x_i^2 + y_i^2 + z_i^2}
\end{equation}

\subsection{Transformation to spherical coordinates}

When we use Quassian quadrature with Laguerre polynomials and MCI with importance
sampling (MCI IS), it is helpful to transform \cref{eq:int} to spherical coordinates.
Transforming $d\*r_1 = dx_1dy_1dz_1$ to spherical coordinates gives us
$r_1^2 dr_1 \sin{\theta_1}d\theta_1 d\phi$ with the new limits $r_1 \in [0, \infty)$,
 $\theta_1 \in [0, 2\pi]$ and $\phi_1 \in [0,\pi]$. The same holds for $d\*r_2$.
[Refer lectures] shows that the distance between $\*r_1$ and $\*r_2$ is given by
\cref{eq:distance_spherical}, with $\cos{\beta}$ defined in \cref{eq:cos_beta}.
This means that the integral in spherical coordinates is \cref{eq:int_spherical}.

\begin{equation}
  \label{eq:int_spherical}
  I_s = \int_{0}^{\infty} \int_{0}^{\infty} \int_{0}^{\pi} \int_{0}^{\pi}
  \int_{0}^{2\pi} \int_{0}^{2\pi}
  dr_1 dr_2 d\theta_1 d\theta_2 d\phi_1 d\phi_2
  r_1^2 r_2^2 \sin{\theta_1} \sin{\theta_2} \frac{e^{-4(r_1 + r_2)}}{r_{12}}
\end{equation}

\begin{equation}
  \label{eq:distance_spherical}
  \frac{1}{r_{12}} = \frac{1}{\sqrt{r_1^2 + r_2^2 - 2r_1r_2 \cos{\beta}}}
\end{equation}

\begin{equation}
  \label{eq:cos_beta}
  \cos{\beta} = \cos{\theta_1} \cos{\theta_2} + \sin{\theta_1} \sin{\theta_2}
  \cos{\phi_1 -\phi_2}
\end{equation}

\subsection{Monte-Carlo Integration}

The main concept of Monte Carlo Integration (MCI) is to evaluate the integrand
at random points in the integration domain.

A crude estimator for the integral $ I = \int_{0}^{1} f(x)dx $ could be $
\hat{I} = f(x)$, with expectation value $\braket{\hat{I}} = \int_{0}^{1}
f(x)p(x)dx$. If $p(x)$ is the uniform distribution $p(x) = 1$ for $x \in
\brak{0,1}$, and $\int_{0}^{1} f(x)p(x) dx = \int_{0}^{1}f(x) dx$. $\hat{I}$ is
therefore an unbiased estimator for $I$.

A better estimator is $\hat{I}_N = \frac{1}{N}\sum_{i=1}^{N} f(x_i)$. It is also
unbiased, and can be shown to have variance
$\sigma_{\hat{I}_N} = \frac{\sigma_{\hat{I}}}{\sqrt{N}}$.

For both the Brute force and importance sampling in several dimensions we will
assume that $p(\*x) = \prod_{i=1}^{d} p(\*x_i)$, with d the number of dimensions,
i.e. that the events are independent.

\subsubsection{Brute Force Monte-Carlo}

To estimate integrals over other intervals we note that the uniform
distribution on $\brak{a,b}$ has probability density $p(x;a,b) = \frac{1}{b-a}$.
To keep the estimator unbiased we have to add a factor $(b-a)$, leading to
$\hat{I}_N = \frac{b-a}{N}\sum_{i=1}^{N}f(x_i)$.
For an integral of dimension d we get

\begin{equation}
  \label{eq:estimator_brute}
  \hat{I}_N
  = \frac{\prod_{j=1}^{d} b_j - a_j }{N}\sum_{i=1}^{N} f(\*x_i)
\end{equation}
with $b_j$ and $a_j$ being the upper and lower limits of the ith integral.

In evaluating \cref{eq:int} we wil use the same limits as in \cref{sec:legendre}
, so our estimator will be

\begin{equation}
  \label{eq:estimator_brute_our}
  \hat{I}_N
  = \frac{\para{2\lambda}^6}{N} \sum_{i=1}^{N} e^{-4(r_{1,i} + r_{2,i})}
\end{equation}

\subsubsection{Importance sampling}

To obtain better estimates of the integral we can reduce the variance by
choosing a better estimator.
One way of doing this is with importance sampling. If we change our estimator to
$\frac{1}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(\*x_i)}$, which is still unbiased, the
variance will decrease if $p(\*x)$ is proportional to $f(\*x)$. We would then sample
more points in the domain where $f(\*x)$ is large than where it is small.
Our integral in spherical coordinates \cref{eq:int_spherical} have an exponential
component in the radial domain. A good distribution for that part would be
the exponential distribution with pdf $p(x) = ae^{-ax}$.
Setting $a = 4$ leads to cancellation of the exponential part of the integral in
spherical coordinates \cref{eq:int_spherical} if we divide by
$p(r_1,r_2) = 4e^{-4r_1} 4e^{-4r_2}$. The new estimator is then
\cref{eq:estimator_importance}. The factor $4\pi^4$ is a correction to the estimator due to sampling from the
uniform distribution with $\theta_1, \theta_2 \in \brak{0, \pi}$ and
$\phi_1, \phi_2 \in \brak{0, 2\pi}$.

\begin{equation}
  \label{eq:estimator_importance}
  \hat{I}_N^p
  = \frac{4\pi^4}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(r1,r2)}
  = \frac{\pi^4}{4N} \sum_{i=1}^{N}
  \frac{r_{1,i}^2 r_{2,i}^2 \sin{\theta_{1,i}} \sin{\theta_{2,i}}}{\abs{r_{12,i}}}
\end{equation}


\subsection{Gaussian Quadrature}
Any quadrature rule can be seen as the as sum of the function we are  
integrating evaluated at specified sample points or integration points $x_i$
multiplied with a weight $w_i$ \cref{eq:quad_sum}.
\begin{equation}\label{eq:quad_sum}
  \int_{a}^{b} f(x)dx \approx \sum_{j=1}^{N} f(x_i) w_i
\end{equation}
For Gaussian Quadrature (GQ) the
integration points are given by the roots of an orthogonal polynomial. The
integration weights are calculated by finding the inverse of a matrix defined by
the orthogonal polynomial \cref{eq:ort_matrix}.       

\begin{equation}\label{eq:ort_matrix}
  L = \begin{bmatrix}
    L_0(x_0) & L_1(x_0) & \dots & L_{N-1}(x_{0}) \\
    L_0(x_1) & L_1(x_1) & \dots & L_{N-1}(x_{1}) \\
    \\
    L_0(x_{n-1}) & L_1(x_{n-1}) & \dots & L_{N-1}(x_{n-1}) 
  \end{bmatrix}
\end{equation}
The different orthogonal polynomial are defined over specific intervals, for
instance Legendre polynomials are defined for $x \in [-1, 1]$. Legendre
polynomials can still be used to for solving integrals over with limits other
than $[-1,1]$ by using transformation of variables. 
\begin{equation}
  t = \frac{b-a}{2}x + \frac{b+a}{2}
\end{equation} 
Then rewrite integral for an interval $[a,b]$,
\begin{equation}
  \int_a^b f(t)dt = \frac{b-a}{2}\int_{-1}^{1} f\left(\frac{b-a}{2}x + \frac{b+a}{2}\right)
\end{equation} 

An issue with using Legendre polynomials to generate our sample points and
weight is that the physical problem \cref{eq:int} we are interested in involves an
integral from $-\infty \to \infty$, which means that we have to make an
approximation. To avoid having to approximate infinity we can instead solve the
transformed version of the integral \cref{eq:int_spherical}, by using Laguerre
polynomial to generate integration points and weights. The Laguerre polynomials
which are defined in the following way.
\begin{equation}\label{eq:laguerre}
  \int_0^{\infty} e^{-\alpha x} f(x)dx \; , \quad \alpha > 0
\end{equation}
Something that is important to note about Laguerre polynomials is that
$e^{-\alpha x}$ is baked into weights which means that we have to divide
\cref{eq:int_spherical} by $e^{-(r_1 + r_2)}$. 

