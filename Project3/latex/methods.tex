\section{Methods}
To be able to calculate the correlation energy, we need an ansatz
\cref{eq:ansatz} for the wave function for the two electrons.
\begin{equation}\label{eq:ansatz}
  \Psi(\bvec{r}_2, \bvec{r}_2) = e^{-\alpha(r_1 + r_2)}
\end{equation}
The integral we will solve is the quantum
mechanical expectation value for the correlation energy \cref{eq:int},
where r$_i$ is the distance from the origin given by \cref{eq:distance}.
Here we have also set $\alpha = 2$. The integral has an analytical solution
equal to $5\pi^2/16^2$ that we will use to evaluate the errors.

\begin{equation}
  \label{eq:int}
  I = \int_{-\infty}^{\infty} d\*r_1 d\*r_2 e^{-4(r_1+r_2)}
  \frac{1}{\abs{\*r_1 - \*r_2}}
\end{equation}

\begin{equation}
  \label{eq:distance}
r_i = \sqrt{x_i^2 + y_i^2 + z_i^2}
\end{equation}

\subsection{Transformation to spherical coordinates}
Before we move onto describing MCI it is useful to transform our integral to
spherical coordinates. Why this is useful will be come clear when we discuss
MCI using importance sampling and GQ using Laguerre polynomials. To give some idea
as why, notice that our integral \cref{eq:int} runs from $-\infty \to \infty$,
this integral is not feasible to integrate without making an approximation of infinity.

Transforming $d\*r_1 = dx_1dy_1dz_1$ to spherical coordinates gives us
$r_1^2 dr_1 \sin{\theta_1}d\theta_1 d\phi$ with the new limits $r_1 \in [0, \infty)$,
 $\theta_1 \in [0, 2\pi]$ and $\phi_1 \in [0,\pi]$. The same holds for $d\*r_2$.
In \cite{lectures} it is shown that the distance between $\*r_1$ and $\*r_2$ is given by
\cref{eq:distance_spherical}, with $\cos{\beta}$ defined in \cref{eq:cos_beta}.
Then the integral in spherical coordinates becomes \cref{eq:int_spherical}.

\begin{equation}
  \label{eq:int_spherical}
  I_s = \int_{0}^{\infty} \int_{0}^{\infty} \int_{0}^{\pi} \int_{0}^{\pi}
  \int_{0}^{2\pi} \int_{0}^{2\pi}
  dr_1 dr_2 d\theta_1 d\theta_2 d\phi_1 d\phi_2
  r_1^2 r_2^2 \sin{\theta_1} \sin{\theta_2} \frac{e^{-4(r_1 + r_2)}}{r_{12}}
\end{equation}

\begin{equation}
  \label{eq:distance_spherical}
  \frac{1}{r_{12}} = \frac{1}{\sqrt{r_1^2 + r_2^2 - 2r_1r_2 \cos{\beta}}}
\end{equation}

\begin{equation}
  \label{eq:cos_beta}
  \cos{\beta} = \cos{\theta_1} \cos{\theta_2} + \sin{\theta_1} \sin{\theta_2}
  \cos{\phi_1 -\phi_2}
\end{equation}

\subsection{Monte-Carlo Integration}

The idea behind Monte Carlo Integration (MCI) is to approximate an integral by
evaluating the integrand at randomly selected points in the integration domain.
Constructing estimators that have expectation value equal to the integral
ensures that the variance of our estimations decreases as the sample size increases.

The random points can be selected in many different ways. \cite{needle} threw
roughly 30000 needles to approximate $\pi$ and \cite{shotgun} used a shotgun.
We will use a random number generator (RNG) to select these points.
For both brute force and importance sampling in several dimensions we will
assume that $p(\*x) = \prod_{i=1}^{d} p(\*x_i)$, with d the number of dimensions,
meaning that the sampling of points are independent events.

We first explain the theory for an integral in one dimension, on the interval 0 to 1,
before generalizing it to other intervals and multi dimensional integrals.

\subsubsection{Brute Force Monte-Carlo}

In the brute force variant of MCI we use the estimator \cref{eq:estimator},
where the random variable X contains N points sampled from the uniform distribution.

\begin{equation}
  \label{eq:estimator}
  \hat{I} = \frac{1}{N}\sum_{i=1}^{N} f(X_i)
\end{equation}

The expectation value of an estimator is $\braket{\hat{I}} = \int_{0}^{1}
\hat{I}p(x)dx$. $p(x)$ is the probability density function (pdf) of the random
variable associated with the estimator. Since $p(x) = 1 $ for $x \in [0,1]$
$\braket{\hat{I}} = \int_0^1 f(x) dx $, showing that \cref{eq:estimator} is an
unbiased estimator of the $\int_0^1 f(x) dx$. The fact that our estimator is
unbiased ensures that our estimator will converge to our integral.


To estimate integrals over other intervals we note that the uniform
distribution on $\brak{a,b}$ has pdf $p(x;a,b) = \frac{1}{b-a}$.
To keep the estimator unbiased we have to add a factor $(b-a)$, leading to
$\hat{I}_N = \frac{b-a}{N}\sum_{i=1}^{N}f(x_i)$.
For an integral of dimension d we get the estimator shown in
\cref{eq:estimator_brute}.

\begin{equation}
  \label{eq:estimator_brute}
  \hat{I}_N
  = \frac{\prod_{j=1}^{d} b_j - a_j }{N}\sum_{i=1}^{N} f(\*x_i)
\end{equation}

with $b_j$ and $a_j$ being the upper and lower limits of the ith integral.

The standard deviation or the error for any MC estimator goes according to \cref{eq: std}

\begin{equation}\label{eq: std}
  \sigma \sim \frac{1}{\sqrt{N}}
\end{equation}


In evaluating \cref{eq:int} we will use the same approximation to infinity
($\infty \approx \lambda$) as when integrating using Legendre polynomials. Our
estimator is then given by \cref{eq:estimator_brute_our}.

\begin{equation}
  \label{eq:estimator_brute_our}
  \hat{I}_N
  = \frac{\para{2\lambda}^6}{N} \sum_{i=1}^{N} e^{-4(r_{1,i} + r_{2,i})}
\end{equation}



\subsubsection{Importance sampling}

To obtain better estimates of the integral we can reduce the variance and
standard deviation of our results by sampling from other distributions and using
another estimator. Instead of sampling X from the uniform distribution we will
sample from a distribution with pdf proportional to the integrand. We would then
evaluate the integrand more often where it is large than where it is small. To
account for this bias we also change our estimator to \cref{eq:new_estimator},
which also have expectation value equal to the integral of $f(x)$.
By scaling with the pdf we weigh the range with high density X and large
function values less.

\begin{equation}
  \label{eq:new_estimator}
  \frac{1}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(\*x_i)}
\end{equation}


Since our integral in spherical coordinates \cref{eq:int_spherical} have an exponential
component in the radial domain we will sample $r_1$ and $r_2$ from the exponential distribution
($p(r) = ae^{-ar}$). Setting a = 4 gives
$p(r_1, r_2) = p(r_1) \cdot p(r_2) = p(r_1,r_2) = 4e^{-4r_1} 4e^{-4r_2}$.
This leads to cancellation of the exponential part of the integral. For the
rest of the integral we will still sample from the uniform distribution,
but now with $\theta_1, \theta_2 \in \brak{0, \pi}$ and
$\phi_1, \phi_2 \in \brak{0, 2\pi}$.
The new estimator is then
\cref{eq:estimator_importance}. The factor $4\pi^4$ is a normalization factor due
to sampling from the uniform distribution on other intervals than $\brak{0,1}$.

\begin{equation}
  \label{eq:estimator_importance}
  \hat{I}_N^p
  = \frac{4\pi^4}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(r_{1,i},r_{2,i})}
  = \frac{\pi^4}{4N} \sum_{i=1}^{N}
  \frac{r_{1,i}^2 r_{2,i}^2 \sin{\theta_{1,i}} \sin{\theta_{2,i}}}{\abs{r_{12,i}}}
\end{equation}


\subsection{Gaussian Quadrature}
Any quadrature rule can be seen as the as sum of the function we are
integrating evaluated at specified sample points or integration points $x_i$
multiplied with a weight $w_i$ \cref{eq:quad_sum}.
\begin{equation}\label{eq:quad_sum}
  \int_{a}^{b} f(x)dx \approx \sum_{j=1}^{N} f(x_i) w_i
\end{equation}
For Gaussian Quadrature (GQ) the
integration points are given by the roots of an orthogonal polynomial. The
integration weights are calculated by finding the inverse of a matrix defined by
the orthogonal polynomial \cref{eq:ort_matrix}.

\begin{equation}\label{eq:ort_matrix}
  L = \begin{bmatrix}
    L_0(x_0) & L_1(x_0) & \dots & L_{N-1}(x_{0}) \\
    L_0(x_1) & L_1(x_1) & \dots & L_{N-1}(x_{1}) \\
    \\
    L_0(x_{n-1}) & L_1(x_{n-1}) & \dots & L_{N-1}(x_{n-1})
  \end{bmatrix}
\end{equation}
The different orthogonal polynomial are defined over specific intervals, for
instance Legendre polynomials are defined for $x \in [-1, 1]$. Legendre
polynomials can still be used to for solving integrals over other intervals
than $[-1,1]$ by using transformation of variables \cref{eq: var_trans}.
\begin{equation}\label{eq: var_trans}
  t = \frac{b-a}{2}x + \frac{b+a}{2}
\end{equation}
Then rewrite integral for an interval $[a,b]$,
\begin{equation}
  \int_a^b f(t)dt = \frac{b-a}{2}\int_{-1}^{1} f\left(\frac{b-a}{2}x + \frac{b+a}{2}\right)
\end{equation}

An issue with using Legendre polynomials to generate our sample points and
weight, is that the physical problem \cref{eq:int} we want solve involves an
integral from $-\infty \to \infty$. To decide on a suitable approximation we
need to determine region the integrand has the largest contribution. For our
integrand \cref{eq:int} the exponential terms goes quickly towards zero as we
move away from the origin. We decided our integrand to be sufficiently close to
zero for $r \pm 2 $.

To avoid having to approximate infinity we can instead solve the
transformed version of the integral \cref{eq:int_spherical}, by using Laguerre
polynomials to generate integration points and weights. The weight function
when using
Laguerre polynomials is defined in the following way \cref{eq:laguerre}.
\begin{equation}\label{eq:laguerre}
 x^{\alpha} e^{-x} \; , \quad 0 \le x \le \infty
\end{equation}
Something that is important to note about Laguerre polynomials is that (for
$\alpha = 0$)
a factor $e^{- x}$ is baked into weights and we have to divide
\cref{eq:int_spherical} by $e^{-(r_1 + r_2)}$. Then we will use Laguerre
polynomials when integrating in the radial domain and use Legendre polynomials
when integrating $\theta$ and $\phi$.



\subsection{Experimental setup}

The code was run on a laptop with the Intel Core i5-8250U, with eight cores
(4 physical cores with hyper threading). For MCI
the number of samples for each experiment was taken on logarithmic intervals,
with 50 different values of sample points between 10 and 10$^{10}$. The GQ
experiments were run with the number of integration points between 1 and 50.
The source code for our implementation of MCI and GQ can be found at our GitHub
\footnote{\url{https://github.com/Ovewh/Computilus/master/Project3}}.
