\section{Methods}
To be able to calculate the correlation energy, we need an ansatz \cref{eq:ansatz} for the
wave function for the two electrons.
\begin{equation}\label{eq:ansatz}
  \Psi(\bvec{r}_2, \bvec{r}_2) = e^{-\alpha(r_1 + r_2)}
\end{equation}
The integral we need to solve is the quatum
mechanical expectation value for the correlation energy \cref{eq:int}, where r$_i$ is the distance from the
origin given by \cref{eq:distance}. Here we have also set $\alpha = 2$. The integral has an analytical solution
equal to $5\pi^2/16^2$ that we will use to evaluate the errors.

\begin{equation}
  \label{eq:int}
  I = \int_{-\infty}^{\infty} d\*r_1 d\*r_2 e^{-4(r_1+r_2)}
  \frac{1}{\abs{\*r_1 - \*r_2}}
\end{equation}

\begin{equation}
  \label{eq:distance}
r_i = \sqrt{x_i^2 + y_i^2 + z_i^2}
\end{equation}

\subsection{Transformation to spherical coordinates}
Before we move onto describing MCI it is useful to transform our integral to
spherical coordinates. Why this is useful will be come clear when we discuss
MCI using importance sampling and GQ using Laguerre polynomials. To give some idea
as why, notice that our integral \cref{eq:int} runs from $-\infty \to \infty$,
this integral is not feasible to integrate without making an approximation of infinity.

Transforming $d\*r_1 = dx_1dy_1dz_1$ to spherical coordinates gives us
$r_1^2 dr_1 \sin{\theta_1}d\theta_1 d\phi$ with the new limits $r_1 \in [0, \infty)$,
 $\theta_1 \in [0, 2\pi]$ and $\phi_1 \in [0,\pi]$. The same holds for $d\*r_2$.
[Refer lectures] shows that the distance between $\*r_1$ and $\*r_2$ is given by
\cref{eq:distance_spherical}, with $\cos{\beta}$ defined in \cref{eq:cos_beta}.
Then the integral in spherical coordinates becomes \cref{eq:int_spherical}.

\begin{equation}
  \label{eq:int_spherical}
  I_s = \int_{0}^{\infty} \int_{0}^{\infty} \int_{0}^{\pi} \int_{0}^{\pi}
  \int_{0}^{2\pi} \int_{0}^{2\pi}
  dr_1 dr_2 d\theta_1 d\theta_2 d\phi_1 d\phi_2
  r_1^2 r_2^2 \sin{\theta_1} \sin{\theta_2} \frac{e^{-4(r_1 + r_2)}}{r_{12}}
\end{equation}

\begin{equation}
  \label{eq:distance_spherical}
  \frac{1}{r_{12}} = \frac{1}{\sqrt{r_1^2 + r_2^2 - 2r_1r_2 \cos{\beta}}}
\end{equation}

\begin{equation}
  \label{eq:cos_beta}
  \cos{\beta} = \cos{\theta_1} \cos{\theta_2} + \sin{\theta_1} \sin{\theta_2}
  \cos{\phi_1 -\phi_2}
\end{equation}

\subsection{Monte-Carlo Integration}

The idea behind Monte Carlo Integration (MCI) is to collect information
about the integral by evaluating the integrand
at random points in the integration domain. As we sample more and more points
the more information we acquire, the more certain we become about our
estimation, and when $N \to \infty$ our estimation equals the integral.  

where $p(x)$ is the probability density function of the random variable

THIS IS A HUGE MESS NEED FIGURING OUT!

A crude estimator for the integral $ I = \int_{0}^{1} f(x)dx $ could be $
\hat{I} = f(X)$, with expectation value $\braket{\hat{I}} = \int_{0}^{1}
f(X)p(X)_XdX$, where $p(X)_X$ is the probability density function of the random
variable $X$. If $p_X(X)$ is the uniform distribution $p(X) = 1$ for $x \in
\brak{0,1}$, and $\int_{0}^{1} f(X)p(X) dX = \int_{0}^{1}f(X) dX$. $\hat{I}$ is
therefore an unbiased estimator for $I$.

A better estimator is $\hat{I}_N = \frac{1}{N}\sum_{i=1}^{N} f(X_i)$. It is also
unbiased, and can be shown to have variance
$\sigma_{\hat{I}_N} = \frac{\sigma_{\hat{I}}}{\sqrt{N}}$.

For both the Brute force and importance sampling in several dimensions we will
assume that $p(\*x) = \prod_{i=1}^{d} p(\*x_i)$, with d the number of dimensions,
i.e. that the events are independent.

\subsubsection{Brute Force Monte-Carlo}

To estimate integrals over other intervals we note that the uniform
distribution on $\brak{a,b}$ has probability density $p(x;a,b) = \frac{1}{b-a}$.
To keep the estimator unbiased we have to add a factor $(b-a)$, leading to
$\hat{I}_N = \frac{b-a}{N}\sum_{i=1}^{N}f(x_i)$.
For an integral of dimension d we get

\begin{equation}
  \label{eq:estimator_brute}
  \hat{I}_N
  = \frac{\prod_{j=1}^{d} b_j - a_j }{N}\sum_{i=1}^{N} f(\*x_i)
\end{equation}
with $b_j$ and $a_j$ being the upper and lower limits of the ith integral.

In evaluating \cref{eq:int} we wil use the same limits as in \cref{sec:legendre}
, so our estimator will be

\begin{equation}
  \label{eq:estimator_brute_our}
  \hat{I}_N
  = \frac{\para{2\lambda}^6}{N} \sum_{i=1}^{N} e^{-4(r_{1,i} + r_{2,i})}
\end{equation}

\subsubsection{Importance sampling}

To obtain better estimates of the integral we can reduce the variance by
choosing a better estimator.
One way of doing this is with importance sampling. If we change our estimator to
$\frac{1}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(\*x_i)}$, which is still unbiased, the
variance will decrease if $p(\*x)$ is proportional to $f(\*x)$. We would then sample
more points in the domain where $f(\*x)$ is large than where it is small.
Our integral in spherical coordinates \cref{eq:int_spherical} have an exponential
component in the radial domain. A good distribution for that part would be
the exponential distribution with pdf $p(x) = ae^{-ax}$.
Setting $a = 4$ leads to cancellation of the exponential part of the integral in
spherical coordinates \cref{eq:int_spherical} if we divide by
$p(r_1,r_2) = 4e^{-4r_1} 4e^{-4r_2}$. The new estimator is then
\cref{eq:estimator_importance}. The factor $4\pi^4$ is a correction to the estimator due to sampling from the
uniform distribution with $\theta_1, \theta_2 \in \brak{0, \pi}$ and
$\phi_1, \phi_2 \in \brak{0, 2\pi}$.

\begin{equation}
  \label{eq:estimator_importance}
  \hat{I}_N^p
  = \frac{4\pi^4}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(r1,r2)}
  = \frac{\pi^4}{4N} \sum_{i=1}^{N}
  \frac{r_{1,i}^2 r_{2,i}^2 \sin{\theta_{1,i}} \sin{\theta_{2,i}}}{\abs{r_{12,i}}}
\end{equation}


\subsection{Gaussian Quadrature}
Any quadrature rule can be seen as the as sum of the function we are
integrating evaluated at specified sample points or integration points $x_i$
multiplied with a weight $w_i$ \cref{eq:quad_sum}.
\begin{equation}\label{eq:quad_sum}
  \int_{a}^{b} f(x)dx \approx \sum_{j=1}^{N} f(x_i) w_i
\end{equation}
For Gaussian Quadrature (GQ) the
integration points are given by the roots of an orthogonal polynomial. The
integration weights are calculated by finding the inverse of a matrix defined by
the orthogonal polynomial \cref{eq:ort_matrix}.

\begin{equation}\label{eq:ort_matrix}
  L = \begin{bmatrix}
    L_0(x_0) & L_1(x_0) & \dots & L_{N-1}(x_{0}) \\
    L_0(x_1) & L_1(x_1) & \dots & L_{N-1}(x_{1}) \\
    \\
    L_0(x_{n-1}) & L_1(x_{n-1}) & \dots & L_{N-1}(x_{n-1})
  \end{bmatrix}
\end{equation}
The different orthogonal polynomial are defined over specific intervals, for
instance Legendre polynomials are defined for $x \in [-1, 1]$. Legendre
polynomials can still be used to for solving integrals over other intervals
than $[-1,1]$ by using transformation of variables.
\begin{equation}
  t = \frac{b-a}{2}x + \frac{b+a}{2}
\end{equation}
Then rewrite integral for an interval $[a,b]$,
\begin{equation}
  \int_a^b f(t)dt = \frac{b-a}{2}\int_{-1}^{1} f\left(\frac{b-a}{2}x + \frac{b+a}{2}\right)
\end{equation}

An issue with using Legendre polynomials to generate our sample points and
weight is that the physical problem \cref{eq:int} that we want solve involves an
integral from $-\infty \to \infty$. To decide on a suitable approximation we
need to find in which region the integrand has the largest contribution. For our
integrand \cref{eq:int} the exponential terms goes quickly towards zero as we
move away from the origin. We decided our integrand to be sufficiently close to
zero for $r \pm 2 $.
  
To avoid having to approximate infinity we can instead solve the
transformed version of the integral \cref{eq:int_spherical}, by using Laguerre
polynomials to generate integration points and weights. The Laguerre polynomials
which are defined in the following way.
\begin{equation}\label{eq:laguerre}
  \int_0^{\infty} e^{-\alpha x} f(x)dx \; , \quad \alpha > 0
\end{equation}
Something that is important to note about Laguerre polynomials is that
$e^{-\alpha x}$ is baked into weights which means that we have to divide
\cref{eq:int_spherical} by $e^{-(r_1 + r_2)}$.



\subsection{Experimental setup}

The code was run on a laptop with eight cores (4 cores with hyper threadings). For the MCI the number of samples
for each experiment was taken on logarithmic intervals, with 40 different values
of sample points between 10 and 10$^9$. The QG experiments were run with the number
of integration points between 2 and 40.
