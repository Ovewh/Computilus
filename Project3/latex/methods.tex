\section{Methods}
To be able to calculate the correlation energy, we need an ansatz \cref{eq:ansatz} for the
wave function for the two electrons.
\begin{equation}\label{eq:ansatz}
  \Psi(\bvec{r}_2, \bvec{r}_2) = e^{-\alpha(r_1 + r_2)}
\end{equation}
The integral we need to solve is the quatum
mechanical expectation value for the correlation energy \cref{eq:int}, where r$_i$ is the distance from the
origin given by \cref{eq:distance}. Here we have also set $\alpha = 2$. The integral has an analytical solution
equal to $5\pi^2/16^2$ that we will use to evaluate the errors.

\begin{equation}
  \label{eq:int}
  I = \int_{-\infty}^{\infty} d\*r_1 d\*r_2 e^{-4(r_1+r_2)}
  \frac{1}{\abs{\*r_1 - \*r_2}}
\end{equation}

\begin{equation}
  \label{eq:distance}
r_i = \sqrt{x_i^2 + y_i^2 + z_i^2}
\end{equation}

\subsection{Transformation to spherical coordinates}
Before we move onto describing MCI it is useful to transform our integral to
spherical coordinates. Why this is useful will be come clear when we discuss
MCI using importance sampling and GQ using Laguerre polynomials. To give some idea
as why, notice that our integral \cref{eq:int} runs from $-\infty \to \infty$,
this integral is not feasible to integrate without making an approximation of infinity.

Transforming $d\*r_1 = dx_1dy_1dz_1$ to spherical coordinates gives us
$r_1^2 dr_1 \sin{\theta_1}d\theta_1 d\phi$ with the new limits $r_1 \in [0, \infty)$,
 $\theta_1 \in [0, 2\pi]$ and $\phi_1 \in [0,\pi]$. The same holds for $d\*r_2$.
\parencite{lectures} shows that the distance between $\*r_1$ and $\*r_2$ is given by
\cref{eq:distance_spherical}, with $\cos{\beta}$ defined in \cref{eq:cos_beta}.
Then the integral in spherical coordinates becomes \cref{eq:int_spherical}.

\begin{equation}
  \label{eq:int_spherical}
  I_s = \int_{0}^{\infty} \int_{0}^{\infty} \int_{0}^{\pi} \int_{0}^{\pi}
  \int_{0}^{2\pi} \int_{0}^{2\pi}
  dr_1 dr_2 d\theta_1 d\theta_2 d\phi_1 d\phi_2
  r_1^2 r_2^2 \sin{\theta_1} \sin{\theta_2} \frac{e^{-4(r_1 + r_2)}}{r_{12}}
\end{equation}

\begin{equation}
  \label{eq:distance_spherical}
  \frac{1}{r_{12}} = \frac{1}{\sqrt{r_1^2 + r_2^2 - 2r_1r_2 \cos{\beta}}}
\end{equation}

\begin{equation}
  \label{eq:cos_beta}
  \cos{\beta} = \cos{\theta_1} \cos{\theta_2} + \sin{\theta_1} \sin{\theta_2}
  \cos{\phi_1 -\phi_2}
\end{equation}

\subsection{Monte-Carlo Integration}

The idea behind Monte Carlo Integration (MCI) is to collect information about
the integral by evaluating the integrand at random points in the integration
domain. We will use a random number generator (RNG) to sample random points \*X
in the integration domain and evaluate the integrand at these points. We construct
estimators that has expectation values equal to the integral.
As the sample size is increased the more certain we become about our estimation, and when the sample size
$N \to \infty$ our estimation equals the integral.

THIS IS A HUGE MESS NEED FIGURING OUT!

We start with a simpler example in one dimension.
A crude estimator for the integral $ I = \int_{0}^{1} f(x)dx $ is
$\hat{I} = f(X)$, where X is a random number from the uniform distribution between
0 and 1. The estimator have expectation value
$\braket{\hat{I}} = \int_{0}^{1} f(x)p(x)dx$, where $p(x)$ is the probability
density function of $X$. If $p(x)$ is the uniform distribution $p(x) = 1$ for $x \in
\brak{0,1}$, and $\int_{0}^{1} f(x)p(x) dX = \int_{0}^{1}f(x) dx$ = I. $\hat{I}$ is
therefore an unbiased estimator for $I$.

A better estimator is $\hat{I}_N = \frac{1}{N}\sum_{i=1}^{N} f(X_i)$. It is also
unbiased, and can be shown to have standard deviation
$\sigma_{\hat{I}_N} = \frac{\sigma_{\hat{I}}}{\sqrt{N}}$.

For both the Brute force and importance sampling in several dimensions we will
assume that $p(\*x) = \prod_{i=1}^{d} p(\*x_i)$, with d the number of dimensions,
meaning that the events are independent.

\subsubsection{Brute Force Monte-Carlo}

To estimate integrals over other intervals we note that the uniform
distribution on $\brak{a,b}$ has probability density function (pdf) $p(x;a,b) = \frac{1}{b-a}$.
To keep the estimator unbiased we have to add a factor $(b-a)$, leading to
$\hat{I}_N = \frac{b-a}{N}\sum_{i=1}^{N}f(x_i)$.
For an integral of dimension d we get

\begin{equation}
  \label{eq:estimator_brute}
  \hat{I}_N
  = \frac{\prod_{j=1}^{d} b_j - a_j }{N}\sum_{i=1}^{N} f(\*x_i)
\end{equation}
with $b_j$ and $a_j$ being the upper and lower limits of the ith integral.

In evaluating \cref{eq:int} we wil use the same approximation to infinity ($\infty \approx \lambda$) as when
integrating using Legendre polynomials. Our estimator is then given by
\cref{eq:estimator_brute_our}.

\begin{equation}
  \label{eq:estimator_brute_our}
  \hat{I}_N
  = \frac{\para{2\lambda}^6}{N} \sum_{i=1}^{N} e^{-4(r_{1,i} + r_{2,i})}
\end{equation}



\subsubsection{Importance sampling}

To obtain better estimates of the integral we can reduce the variance and
standard deviation by sampling from other distrubutions and using another estimator.
Instead of sampling X from the uniform distribution we will sample from a
distribution with pdf proportional to the integrand.
We would then evaluate the integrand more often where it is large than where it
is small. To account for this bias we also change our estimator to
\cref{eq:new_estimator}, which also have expectation value $\int_{-\infty}^{\infty} f(\*x) d\*x$.
Dividing by the pdf we weigh the parts where the function has large values and were
X is dense less.

\begin{equation}
  \label{eq:new_estimator}
  \frac{1}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(\*x_i)}
\end{equation}


Since our integral in spherical coordinates \cref{eq:int_spherical} have an exponential
component in the radial domain we will sample $r_1$ and $r_2$ from the exponential distribution
($p(r) = ae^{-ar}$). Setting a = 4 gives
$p(r_1, r_2) = p(r_1) \cdot p(r_2) = p(r_1,r_2) = 4e^{-4r_1} 4e^{-4r_2}$.
This leads to cancellation of the exponential part of the integral. For the
rest of the integral we will still sample from the uniform distribution,
but now with $\theta_1, \theta_2 \in \brak{0, \pi}$ and
$\phi_1, \phi_2 \in \brak{0, 2\pi}$.
The new estimator is then
\cref{eq:estimator_importance}. The factor $4\pi^4$ is a correction to the
estimator due to sampling from the uniform distribution.

\begin{equation}
  \label{eq:estimator_importance}
  \hat{I}_N^p
  = \frac{4\pi^4}{N} \sum_{i=1}^{N} \frac{f(\*x_i)}{p(r1,r2)}
  = \frac{\pi^4}{4N} \sum_{i=1}^{N}
  \frac{r_{1,i}^2 r_{2,i}^2 \sin{\theta_{1,i}} \sin{\theta_{2,i}}}{\abs{r_{12,i}}}
\end{equation}


\subsection{Gaussian Quadrature}
Any quadrature rule can be seen as the as sum of the function we are
integrating evaluated at specified sample points or integration points $x_i$
multiplied with a weight $w_i$ \cref{eq:quad_sum}.
\begin{equation}\label{eq:quad_sum}
  \int_{a}^{b} f(x)dx \approx \sum_{j=1}^{N} f(x_i) w_i
\end{equation}
For Gaussian Quadrature (GQ) the
integration points are given by the roots of an orthogonal polynomial. The
integration weights are calculated by finding the inverse of a matrix defined by
the orthogonal polynomial \cref{eq:ort_matrix}.

\begin{equation}\label{eq:ort_matrix}
  L = \begin{bmatrix}
    L_0(x_0) & L_1(x_0) & \dots & L_{N-1}(x_{0}) \\
    L_0(x_1) & L_1(x_1) & \dots & L_{N-1}(x_{1}) \\
    \\
    L_0(x_{n-1}) & L_1(x_{n-1}) & \dots & L_{N-1}(x_{n-1})
  \end{bmatrix}
\end{equation}
The different orthogonal polynomial are defined over specific intervals, for
instance Legendre polynomials are defined for $x \in [-1, 1]$. Legendre
polynomials can still be used to for solving integrals over other intervals
than $[-1,1]$ by using transformation of variables.
\begin{equation}
  t = \frac{b-a}{2}x + \frac{b+a}{2}
\end{equation}
Then rewrite integral for an interval $[a,b]$,
\begin{equation}
  \int_a^b f(t)dt = \frac{b-a}{2}\int_{-1}^{1} f\left(\frac{b-a}{2}x + \frac{b+a}{2}\right)
\end{equation}

An issue with using Legendre polynomials to generate our sample points and
weight is that the physical problem \cref{eq:int} that we want solve involves an
integral from $-\infty \to \infty$. To decide on a suitable approximation we
need to find in which region the integrand has the largest contribution. For our
integrand \cref{eq:int} the exponential terms goes quickly towards zero as we
move away from the origin. We decided our integrand to be sufficiently close to
zero for $r \pm 2 $.

To avoid having to approximate infinity we can instead solve the
transformed version of the integral \cref{eq:int_spherical}, by using Laguerre
polynomials to generate integration points and weights. The Laguerre polynomials
which are defined in the following way.
\begin{equation}\label{eq:laguerre}
  \int_0^{\infty} e^{-\alpha x} f(x)dx \; , \quad \alpha > 0
\end{equation}
Something that is important to note about Laguerre polynomials is that
$e^{-\alpha x}$ is baked into weights which means that we have to divide
\cref{eq:int_spherical} by $e^{-(r_1 + r_2)}$.



\subsection{Experimental setup}

The code was run on a laptop with Intel Core i5-8250U, with 4 cores. For the MCI
the number of samples for each experiment was taken on logarithmic intervals,
with 50 different values of sample points between 10 and 10$^10$. The QG
experiments were run with the number of integration points between 1 and 50.
The implementation of MCI and GQ can be found at our github \parencite{github}.
