\section{Results}

\subsection{Monte-Carlo integration}

\subsubsection{Error and accuracy}

\Cref{fig:mc_error} shows the absolute value of the difference between the
calculated value of the integral and the analytical solution with brute force
and importance sampling. N is the number of samples taken. It is worth to notice the sometimes
large differences in error from one run to the next, due to the stochastic nature of
MCI. Still the improvement in accuracy when using importance sampling
is significant especially for larger samples sizes when the standard
deviation is smaller. Yielding an improvement of a factor 10 or more for sample
sizes greater that $10^8$. 


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/mc_error.png}
  \caption{Absolute value of difference between Monte-Carlo results and analytical
  solution using brute force and importance sampling.}

  \label{fig:mc_error}
\end{figure}

To look into the dependence of accuracy on runtime we show \cref{fig:mc_std_time}
which shows standard deviation divided by run time for several N. Taking account
for the run time shows that the importance sampling in this case gets really
effective for $N \geq 10^5$, outperforming the brute force by roughly a factor
40.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/mc_std_time.png}
  \caption{Standard deviation and run time.}

  \label{fig:mc_std_time}
\end{figure}

\subsubsection{Parallelization}

The results of parallelizing our code is shown in \cref{fig:mc_time_ratio}, which
shows that when N is larger than 10000 we acheived a speedup in the order of 5.
Since the code was run on a laptop with 8 cores this was not optimal, but still
a great boost to the run time.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/mc_time_ratio.png}
  \caption{Run time of unparallelized divided by parallelized version
          of the importance sampling algorithm.}

  \label{fig:mc_time_ratio}
\end{figure}


\subsection{Gaussian Quadrature}

The error of the GQ methods \cref{fig:gauss_error} shows that GQLeg gives relatively
low(high) errors when the number of integration points are odd(even). For GQLeg
the error seems to converge around roughly \num{ 5e-3}, and never reaches the
desired error of $10^{-3}$. The GQLag do reach the desired error at N = 14 and 15,
but for higher Ns the error converges to \num{2e-3}, and does not show the oscillatory
nature of the GQLeg.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/gauss_error.png}
  \caption{Absolute value of difference between gaussian quadrature results and analytical
  solution using Legendre and Laguerre polynomials..}

  \label{fig:gauss_error}
\end{figure}


\subsection{Comparison of Gaussian Quadrature and Monte-Carlo Integration}

To clarify whether running Gaussian Quadrature or Monte-Carlo Integration would
give the best results for a given run time we made error-runtime plots for all
of the methods we have used \cref{fig:time_compare}. For MCI low values of runtime
corresponds to few samples, and for GQ it corresponds to a small number of integration points.
For the shortest runtimes the results of MCI were a bit all over the place, but
looking at longer runtimes we see a clear trend of the error decreasing, especially
for the importance sampling.


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/time_compare.png}
  \caption{Absolute values of the error as a function of time for all methods we
  used to approximate the integral.}

  \label{fig:time_compare}
\end{figure}
