\section{Methods}

\subsection*{The Jacobi's Method}

The physical problem we will look at is the one dimensional wave function
\begin{equation}
\gamma \frac{d^2 u(x)}{dx^2} = -F u(x)
\end{equation}
with u(x) being the vertical displacement of a beam in the y direction, where
x $\in [0, L]$, with L is the length of the beam. $\gamma$ is a material constant,
and F is a force towards the origin being applied at the right hand side of the beam.
We will use the boundary conditions u(0) = u(L) = 0, meaning that the beam is
fixed at the end points.


Since we want to solve this equation numerically we scale it.

Introducing $ \rho = \frac{x}{L} $ limits $\rho$ to $[0, 1]$ and scales the
equation to
$$\frac{d^2 u(\rho)}{d\rho^2} = - \frac{FL^2}{\gamma} u(\rho) = -\lambda u(\rho).$$

We discretize $\rho$ on a grid with N+1 points, which
define the step length as $h = \frac{\rho_N - \rho_0}{N}$ with $\rho_N = 1$ and
$\rho_0 = 0$.

Using the three point formula for the second derivative $ u'' = \frac{u(\rho + h) - 2u(\rho) +
u(\rho -u))}{h^2} + O(h^2)$, the initial equation can be discretized as
$\frac{-u_{i+1} + 2u_i - u_{i-1}}{h^2} = \lambda u_i$
or, in matrix form
$ A \vec{u} = \lambda \vec{u}$
with A being a tridiagonal matrix with $a = \frac{-1}{h^2}$ on the upper and lower
diagonal, and $d = \frac{2}{h^2}$ on the diagonal. $\vec{u}^T = [u_1, u_2, ..., u_{n-1}]$

Our equation is now in the form of an eigenvalue problem, which in this case has
analytical eigenvalues $\lambda_j = d + 2a\cos{(\frac{j\pi}{n+1})}$ for $j \in
[1,2,...,n]$ where n is the size of A. \cite{lectures}

\subsection{Properties of orthogonal transformations}

An orthogonal transformation U has the property $U^T U = U U^T = I$.

Assuming we have an orthonormal basis consisting of $\vec{v}_i^T = [v_{1i},
v_{2i}, ... , v_{ni}]$ we know that $\vec{v}_i^T \vec{v}_j = \vec{v}_i \cdot
\vec{v}_j = \delta_{ij}$. Let $\vec{w}_i = U\vec{v}_i$. $\vec{w}_i^T \vec{w}_j =
(U\vec{v}_i)^T(U\vec{v}_j) = \vec{v}_i^T U^T U \vec{v}_j = \vec{v}_i^T \vec{v}_j
= \delta_{ij}$. This shows that a orthogonal transformation preserves the dot
product and orthogonality.

\subsection{Jacobi´s method}

The idea behind Jacobi´s method is to diagonalize A by applying an orthogonal
rotation R$^T$ a repeated number of times. By choosing a specific angle $\theta$
we can zero out one element of the transformed matrix. We will use the shorthand
c = $\cos{\theta}$, s = $\sin{\theta}$ and t = $\tan{\theta}$ where $\theta$ is
the angle of rotation.


$$R(k,l,\theta) =
\begin{bmatrix}
  1       & \cdots  & 0       & \cdots  & 0       & \cdots  & 0 \\
  \vdots  & \ddots  &  \vdots &         & \vdots  &         & \vdots \\
  0       & \cdots  & c       & \cdots  & -s      & \cdots  & 0 \\
  \vdots  &         & \vdots  & \ddots  & \vdots  &         & \vdots\\
  0       & \cdots  & s       & \cdots  & c       & \cdots  & 0\\
  \vdots  &         & \vdots  &         &  \vdots & \ddots  & \vdots\\
  0       & \cdots  & 0       & \cdots  & 0       & \cdots  & 1
\end{bmatrix}
$$
where k and l is the row number containing c and -s, and s and c respectively.


After one transformation $R^T A R R^T \vec{x} = B (R^T \vec{x}) = \lambda (R^T
\vec{x})$ we see that the new eigenvector is $R^T\vec{x}$.

Performing the transformation results in the following matrix.
(refer compendium 216)

\begin{align*}
  b_{ii} &= a_{ii}, i \neq k, i \neq \\
  b_{ik} &= a_{ik}c - a_{il}s, i \neq k, i \neq \\
  b_{il} &= a_{il}c + a_{ik}s, i \neq k, i \neq \\
  b_{kk} &= a_{kk}c^2 - 2a_{kl}cs + a_{ll}s^2 \\
  b_{ll} &= a_{ll}c^2 + 2a_{kl}cs + a_{kk}s^2 \\
  b_{kl} &= (a_{kk}- a_{ll})cs + a_{kl}(c^2 - s^2)
\end{align*}

Since A is symmetric, the orthogonal transformation of A will also be symmetric
which leads to
\begin{align*}
  b_{ki} &= b_{ik} \\
  b_{li} &= b_{il} \\
  b_{lk} &= b_{lk}
\end{align*}

From this we see that the only changes between B and A will be in the columns
and rows k,l, and we will not have to perform the full matrix multiplication.

Setting b$_{kl}$ equal to zero and defining $\tau = \frac{a_{kk} - a_{ll}}{2a_{kl}}$
gives

\begin{align*}
  \tau &= -\frac{cos(2\theta)}{sin(2\theta)} \\
  &=  \frac{1}{tan(2\theta)}
\end{align*}

Solving for t leads to $t^2 + 2t\tau -1 = 0$, with roots $t = -\tau \pm \sqrt{(\tau^2 + 1)}$. We will
use this equation to fix $\theta$ to the angle that eliminates element a$_{kl}$.
Having fixed the angle we find $c = \frac{1}{\sqrt{(1+t^2)}}$ and $s = tc$, which
we need to build the transformation matrix.

We need a strategy for choosing which element to eliminate. Our goal is to
transform A into a diagonal matrix, meaning all offdiagonal elements are zero
(or very close to zero). We can achieve this by minimizing the sum of the
squared nondiagional elements, off(B)$^2$. Since the frobenius norm is
preserved under an orthogonal transformation we must have
$$ \abs{B}_F^2 = \abs{A}_F^2 =  off(B)^2 + \sum_{i}^{n} b_{ii} = off(A)^2 + \sum_{i}^{n} a_{ii}.$$

Utilizing this we show that

\begin{align*}
  off(B)^2 &= \abs{B}_F^2 - \sum_{i=1}^{n} b_{ii}^2 \\
  &= \abs{A}_F^2 - \sum_{i=1}^{n} b_{ii}^2 \\
  &= \abs{A}_F^2 - \sum_{i \neq k \neq l}^{n} b_{ii}^2 - (b_{kk}^2 + 2b_{kl}^2 + b_{ll}^2) \\
  &= \abs{A}_F^2 - \sum_{i\neq k \neq l}^{n} a_{ii}^2 - (a_{kk}^2 + 2a_{kl}^2 + a_{ll}^2) \\
  &= \abs{A}_F^2 - \sum_{i=1}^{n} a_{ii}^2 - 2a_{kl}^2 \\
  &= off(A)^2 - 2a_{kl}^2
\end{align*}

meaning the difference in the off(A)$^2$ and off(B)$^2$ is the greatest
when we choose k and l to correspond with the largest offdiagonal element in A.

Our algorithm is then:

\begin{algorithmic}
\While{$a_{ij}^2 > \epsilon$}
    \State calculate: $\tau$, t, s, c
    \State Perform transformation
    \State Find largest nondiagonal element
  \EndWhile
  \State Read off diagonal elements to find eigenvalues
\end{algorithmic}


Our implementation of the jacobi method can be found at our github \cite{github}.

\subsection*{Experimental setup}
To have our benchmark consistent every run was conducted using the same pc. We
ran each implementation was ran five times for different N ranging from 0 to
100. We also ran a python version for the same values of N. For N larger than
100 the python version was so slow that we decided to only run the other
implementations. To investigate performance for large N we ran the c++, Numba
and Cython implementations for N ranging from 100 to 350 and for each N we
ran the program five times. We only timed the part of the code which calculated
the eigenvalues.
