\section{Methods}

Conceptually these systems are quite simple to model,
consisting of binary spins which can either have the value 1 (up) or -1 (down).
The energy of this system is given by summing up nearest neighbour spins of all
spins in our system. We evolve our system by randomly selecting spins and
changing their values if it is energetically favourable for the system. To not
have our system being stuck in the ground state (lowest energy state) we will randomly flip some spins
anyway even if it is not energetically favourable, this behaviour is actually
something for real systems to.
\subsection{Ising model}
The Ising model, without an external magnetic field, is given by
\cref{eq:ising}, where $E_i$ is the energy of a given microstate s$_k$ and s$_l$ takes values of $\pm 1$, O is the
number of spins, J is a coupling constant corresponding to the strength of the
interaction between the neighbouring spins and $<kl>$ means that we sum over
nearest neighbour only.

\begin{equation}\label{eq:ising}
  E_i = -J\sum_{<kl>}^{O} s_k s_l
\end{equation}
The parameter that we can control in our experiment is the temperature it is
therefore it is natural to
use the canonical ensemble where temperature is the intensive property, meaning
that it is a local property that does not depend on the size of the system. The
mean energy of the system is then given as an expectation value. To calculate
the expectation value for lets say the energy, we need a probability distribution
for a given temperature. In the canonical ensemble the probability distribution
is given by the Boltzmann distribution \cref{eq:Boltzmann}

\begin{equation}\label{eq:Boltzmann}
  P_i(\beta) = \frac{e^{-\beta E_i}}{Z}
\end{equation}

$\beta$ is the inverse temperature times the Boltzmann constant
($\beta = 1/k_b*T $),  $E_i$ is the energy of a microstate and $Z$
\cref{eq:partFunc} is the partition function for the canonical ensemble.

\begin{equation}\label{eq:partFunc}
  Z = \sum_{i=1}^{N} e^{-\beta E_i}
\end{equation}

The properties which we are interested in, the ,energy $\braket{E}$, heat
capacity $C_v$,magnetisation $\braket{M}$ and the susceptibility $\chi$ are then given
by the following expressions.
\begin{eqnarray}
  \braket{E} = \frac{1}{Z} \sum_{i=1}^N E_i e^{\beta E_i} \\
  C_v = \frac{1}{k_B T^2}\left(\braket{E^2} - \braket{E}^2 \right) \\
  \braket{M} = \frac{1}{Z} \sum_{i=1}^{N} M e^{-\beta E_i} \\
  \chi = \frac{1}{k_b T}\left(\braket{M^2}- \braket{M}^2\right)
\end{eqnarray}

The variance of the energy and magnetisation is defined as the following.
\begin{eqnarray}
  \sigma_E^{2} = \left(\braket{E^2} - \braket{E}^2 \right) = \frac{1}{Z}
  \sum_{i=1}^N E_i^2 e^{\beta E_i} - \left(\frac{1}{Z} \sum_{i=1}^N E_i e^{\beta
  E_i}\right)^2  \\
  \sigma_M^{2} = \left(\braket{M^2}- \braket{M}^2\right) = \frac{1}{Z} \sum_{i=1}^{N} M_i^2 e^{-\beta E_i}  - \left(\frac{1}{Z} \sum_{i=1}^{N} M e^{-\beta E_i}  \right)^2
\end{eqnarray}


\subsection{Metropolis algorithm}
The issue with our current setup of the Ising Model is that our probability
distribution, which we need in order to calculate the expectation values
contains the partition function $Z$. To calculate the partition function
requires on the order $2^{O}$,FLOPS where $O$ is number of microstates and
for large lattices it becomes impossible compute. We need to devise a scheme
that lets us obtain the probability distribution without having to calculate the
partition function. To achieve this end we will apply three famous concepts in
stochastic modeling, Monte Carlo sampling, Markov chains, and the Metropolis
algorithm .

After we have setup the initial state, the first step in devising our
stochastic Ising model would be to setup our Monte Carlo sampler or random walk.
The random walk will randomly select spins in
our lattice. For each of the randomly selected spins we calculate the energy and
then make a proposal about whether to flip the orientation of the spin.

We will
accept the proposal if flipping the spin results in lowering the energy of the
system. Still we there is an issue with only accepting the proposals that lowers
the energy, which is that after some steps we would start refusing every
subsequent proposals. Since all of the proposed moved would result in a higher
energy. This would eventually lead to us getting stuck in the lowest energy
state. To avoid getting stuck we would need to also accept some proposals which
results in a higher energy of the system. This is where the Metropolis
algorithm comes to our rescue. The first step of the Metropolis algorithm is to
draw a random number between 0 and 1 from a probability distribution, we will
use the uniform distribution. Next is to check if the our random number $r$,  $r \leq P_i
/P_j$ where $P_i$ is the probability distribution of our new state and $P_j$ is
the probability distribution of the previous state. If we recall that the
probability distribution in both states are given by the Boltzmann distribution
\cref{eq:Boltzmann}, the probability ratio $P_i/P_j$ then becomes \cref{eq:pRatio}.

\begin{equation}\label{eq:pRatio}
  \frac{P_i}{P_j} = \frac{e^{-\beta E_i}/Z}{e^{-\beta E_j}/Z} = e^{-\beta \Delta E} \; , \quad \Delta E = E_i - E_j
\end{equation}
It worth noticing that the probability ratio does not depend on the partition
function. The Metropolis rule for which accept or reject proposed moves then
becomes $r \leq e^{-\beta \Delta E}$.  $e^{-\beta \Delta E}$ is something we
pre calculate for a given temperature, since for the Ising
model the energy difference can only take on five different values \cref{eq:deltaE}.
\begin{equation}\label{eq:deltaE}
  \Delta E = -8J, \; -4J, \; 0, \; 4J, \; 8J
\end{equation}
This is quite straight forward to verify by using \cref{eq:ising} to calculate
the energy of the original state and the new proposed
state and then finding the difference. Pre calculating the energy saves us a lot
of CPU cycles as we do not need to calculate an exponential each time we propose
a new state. There is a neat physical interpretation to the Metropolis
algorithm as models the struggle between two fundamental principles of our
universe, energy minimization and entropy maximization. This is something we
will observe in our model as the entropy or acceptance ratio increases as we
increase the temperature.


\subsection{Analytic solution of the Ising Model for a 2x2 lattice}
In reality it is impossible to calculate the partition function, because you can
have infinite number of microstates. Still for a small lattice 2x2, which only
has 16 microstates it is
feasible to find analytic solutions for the values of
interest. Comparing with the analytical expressions will be a useful test of our
numerical estimates. For details see \cref{app:analytic}.

We start by calculating the energy for a given microstate.

\begin{equation}
  E_i = -2J\brak{(s_4 + s_1)(s_2+s_3)}
\end{equation}

To calculate the expectation values we need to know the possible energies and
magnetizations. The script analytic.py
\footnote{\url{https://github.com/Ovewh/Computilus/tree/master/Project4}}
generates all the possible states and lists the values we need as shown in
\cref{tab:analytic}.

\begin{table}[htp]
  \centering
  \csvautotabular{../data/analytic.csv}
  \caption{Analytical values for a 2x2 grid.}
  \label{tab:analytic}
\end{table}

The probability of a given microstate is given by \cref{eq:prob}, with the z
being a normalization factor to ensure the sum of probabilities are one, known
as the partition function, given in \cref{eq:partition}.


\begin{equation}
  \label{eq:prob}
  P_i = \frac{\exp(-\beta E_i)}{z}
\end{equation}

\begin{equation}
  \label{eq:partition}
  z = \sum_{i}^{N} \exp(-\beta E_i) = \exp(8\beta) + 12 + 2\exp(-8\beta)
\end{equation}


\section{Experimental Setup}
We will
use quadratic grids with grid width L and periodic
boundary conditions.


\subsection{Equilibrium}

When we do the production runs we want to start calculating expectation values
when the system has reached equilibrium, or the most likely state. A crude
method for estimating this delay value (\# of MC cycles) is to plot energy and
magnetization against MC cycles. We did this for T $\in \brak{1.0,2.4}$ for both
ordered (all spins pointing up) and unordered initial states. From
\cref{fig:equi_E} and \cref{fig:equi_M} it is clear that the system takes longer
to reach the most likely state when the initial state is unordered. The
magnetisation seems to take longer to stabilize than the energy. At around 7500
cycles all values seem to be stable, showing small oscillations. Since we will
be running several hundred thousands MC cycles we played it safe and set the
delay parameter to 50000 in the production runs.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/equilibrium_E.pdf}
  \caption{Absolute values of energy for different MC cycles.
  ord refers to the initial microstate of spins. 1=up, 0=random.}
  \label{fig:equi_E}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/equilibrium_Mabs.pdf}
  \caption{Absolute values of magnetization for different MC cycles.
  ord refers to the initial microstate of spins. 1=up, 0=random.}
  \label{fig:equi_M}
\end{figure}




\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../figures/accepted.pdf}
  \caption{Accepted microstates scaled with number of spins and MC cycles.}
  \label{fig:accepted}
\end{figure}

\subsection{Model testing and error analysis}

Implementation of tests (test\_analytic.py, test\_ising.py) showed that the
analytical solutions was correct down to an absolute error of \num{1e-12}  when
comparing against a brute force method where we created all the possible
microstates.


To test the implementation of the ising model we calculated the relative error
in energy, magnetization, heat capacity and susceptibility (\cref{fig:error_L2})
for L=2 (where we have calculated analytical solutions). The relative error
was proportional to $1/ \textbf{MC cycles}^{-1/2}$.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../figures/relative_error.pdf}
  \caption{Relative error vs MC cycles for L=2 for two different temperatures. Ordered
  refers to the inital spin configuration, with 1 meaning all spins pointing up and
  0 meaning a random configuration. Plots are smoothed by taking a
  rolling mean with window size 5. The relative error decreases roughly as
  $\sqrt{\textbf{MC cycles}}$}
  \label{fig:error_L2}
\end{figure}


\subsection{Production runs}

For the production runs (phase\_transitions.py) we parallelized the code with
numba using $@njit(parallel=True)$. We found it easiest to run calculations for
each temperature in parallel, instead of parallelizing the ising model itself.
An unfortunate side effect of this was that we were not able to write to file
after each run of the ising model, so if our run did not finish we would lose
all the results. We also added a script (timings.py) that estimates the time to
run phase\_transitions.py with the given parameters so we did not start too time
consuming runs.

Timings with and without parallelization for small grid sizes showed a speedup
of roughly 3.4 on a laptop with 4 physical cores. The suboptimal speedup might
be related to creation of and writing to the 2d arrays for storing results,
but we did not look deeper into this.

\subsection{Probability distribution}

\begin{figure}[ht]
  \begin{subfigure}[t]{\textwidth} % top align
    \centering
    \includegraphics[width=\linewidth]{../figures/distribution_0.pdf}
    \caption{}
    \label{fig:sub-first}
  \end{subfigure}
  \hfill
  \newline
  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=\linewidth]{../figures/distribution_5.pdf}
    \caption{}
    \label{fig:sub-second}
  \end{subfigure}
  \label{fig:distribution}
  \caption{Probability distribution of energy, scaled with number of spins. L=20
  MC cycles = \num{1e6}.}
\end{figure}


The probability distribution of energy (\cref{fig:distribution}) shows that
at low temperature almost all of the states are in a low energy state.
As the temperature and the variance increases the distribution becomes more
spread out.
