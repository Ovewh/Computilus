\section{Methods}

The

Conceptually these systems are quite simple to model,
consisting of binary spins which can either have the value 1 (up) or -1 (down).
The energy of this system is given by summing up nearest neighbour spins of all
spins in our system. We evolve our system by randomly selecting spins and
changing their values if it is energetically favourable for the system. To not
have our system being stuck in the ground state (lowest energy state) we will randomly flip some spins
anyway even if it is not energetically favourable, this behaviour is actually
something for real systems to.
\subsection{Ising model}
The Ising model, without an external magnetic field, is given by
\cref{eq:ising}, where $E_i$ is the energy of a given microstate s$_k$ and s$_l$ takes values of $\pm 1$, O is the
number of spins, J is a coupling constant corresponding to the strength of the
interaction between the neighbouring spins and $<kl>$ means that we sum over
nearest neighbour only.

\begin{equation}\label{eq:ising}
  E_i = -J\sum_{<kl>}^{O} s_k s_l
\end{equation}
The parameter that we can control in our experiment is the temperature it is
therefore it is natural to
use the canonical ensemble where temperature is the intensive property, meaning
that it is a local property that does not depend on the size of the system. The
mean energy of the system is then given as an expectation value. To calculate
the expectation value for lets say the energy, we need a probability distribution
for a given temperature. In the canonical ensemble the probability distribution
is given by the Boltzmann distribution \cref{eq:Boltzmann}

\begin{equation}\label{eq:Boltzmann}
  P_i(\beta) = \frac{e^{-\beta E_i}}{Z}
\end{equation}

$\beta$ is the inverse temperature times the Boltzmann constant
($\beta = 1/k_b*T $),  $E_i$ is the energy of a microstate and $Z$
\cref{eq:partFunc} is the partition function for the canonical ensemble.

\begin{equation}\label{eq:partFunc}
  Z = \sum_{i=1}^{N} e^{-\beta E_i}
\end{equation}

The properties which we are interested in, the ,energy $\braket{E}$, heat
capacity $C_v$,magnetisation $\braket{M}$ and the susceptibility $\chi$ are then given
by the following expressions.
\begin{eqnarray}
  \braket{E} = \frac{1}{Z} \sum_{i=1}^N E_i e^{\beta E_i} \\
  C_v = \frac{1}{k_B T^2}\left(\braket{E^2} - \braket{E}^2 \right) \\
  \braket{M} = \frac{1}{Z} \sum_{i=1}^{N} M e^{-\beta E_i} \\
  \chi = \frac{1}{k_b T}\left(\braket{M^2}- \braket{M}^2\right)
\end{eqnarray}

The variance of the energy and magnetisation is defined as the following.
\begin{eqnarray}
  \sigma_E^{2} = \left(\braket{E^2} - \braket{E}^2 \right) = \frac{1}{Z}
  \sum_{i=1}^N E_i^2 e^{\beta E_i} - \left(\frac{1}{Z} \sum_{i=1}^N E_i e^{\beta
  E_i}\right)^2  \\
  \sigma_M^{2} = \left(\braket{M^2}- \braket{M}^2\right) = \frac{1}{Z} \sum_{i=1}^{N} M_i^2 e^{-\beta E_i}  - \left(\frac{1}{Z} \sum_{i=1}^{N} M e^{-\beta E_i}  \right)^2
\end{eqnarray}


\subsection{Metropolis algorithm}
The issue with our current setup of the our Ising Model is that our probability
distribution, which we need in order to calculate the expectation values
contains the partition function $Z$. To calculate the partition requires on the
order $2^{O}$ flops and
for large lattices it becomes impossible compute. We need to devise a scheme
that lets us obtain the probability distribution without having to calculate the
partition function. To achieve this end we will apply three famous concepts in
stochastic modeling, Monte Carlo sampling, Markov chains, and the Metropolis
algorithm .

After we have setup the initial microstate, the first step in devising our stochastic Ising model would be to setup our
Monte Carlo sampler or random walk. The random walk will random select spins in
our lattice. For each of the randomly selected spins we calculate the energy and
then make a proposal about whether to flip the orientation of the spin. We can
pre calculate the energy for a given temperature energy since for the Ising
model the energy difference can only take on five different values \cref{eq:deltaE}.
\begin{equation}\label{eq:deltaE}
  \Delta E = -8J, \; -4J, \; 0, \; 4J, \; 8J
\end{equation}
This is quite straight forward to verify by using \cref{eq:ising} to calculate
the energy of the original spins configuration and the new proposed
configuration and then calculating the difference.

We will
accept the proposal if flipping the spin results in lowering the energy of the
system. Still we there is an issue with only accepting the proposals that lowers
the energy, which is that after some steps we would start refusing every
subsequent proposals. Since all of the proposed moved would result in a higher
energy. This would eventually lead to us getting stuck in the lowest energy
state. To avoid getting stuck we would need to also accept some proposals which
results in a higher energy of the system. This is where the Metropolis
algorithm comes to our rescue. The first step of the Metropolis algorithm is to
draw a random number between 0 and 1 from a probability distribution, we will
use the uniform distribution. Then we check if the our random number $r \leq P_i
/P_j$ where $P_i$ is the probability distribution of our new state and $P_j$ is
the probability distribution of the previous state. If we recall that the
probability distribution in both states are given by the Boltzmann distribution
\cref{eq:Boltzmann}, the probability ratio $P_i/P_j$ then becomes \cref{eq:pRatio}.

\begin{equation}\label{eq:pRatio}
  \frac{P_i}{P_j} = \frac{e^{-\beta E_i}/Z}{e^{-\beta E_j}/Z} = e^{-\beta \Delta E} \; , \quad \Delta E = E_i - E_j
\end{equation}
It worth note that the probability ratio does not depend on the partition
function. The new Metropolis acceptance rule then becomes the $r \leq e^{-\beta
\Delta E}$.  We can
pre calculate the energy for a given temperature energy since for the Ising
model the energy difference can only take on five different values \cref{eq:deltaE}.
\begin{equation}\label{eq:deltaE}
  \Delta E = -8J, \; -4J, \; 0, \; 4J, \; 8J
\end{equation}
This is quite straight forward to verify by using \cref{eq:ising} to calculate
the energy of the original state and the new proposed
state and then finding the difference. Pre calculating the energy saves us a lot
of CPU cycles as we do not need to calculate an exponential each time we propose
a new state. Metropolis tweak to our proposal acceptance rule
encapsulates the eternal struggle between two fundamental principles of our
universe, energy minimization and entropy maximization.


\subsection{Analytic solution of the Ising Model for a 2x2 lattice}
In reality it is impossible to calculate the partition function, because you can
have infinite number of microstates. Still for a small lattice 2x2, which only
has 16 microstates it is
feasible to find analytic solutions for the values of
interest. Comparing with the analytical expressions will be useful test of our
numerical estimates.

We start by calculating the energy for a given microstate.

\begin{align*}
  E_i &= -J \sum_{<k,l>} s_k s_l = -\frac{1}{2}J \sum_{i} \sum_{nn} s_i s_{nn} \\
      &= -\frac{1}{2} J \brak{2 s_1 (s_2 + s_3) + 2s_2(s_1 + s_4) + 2s_3(s_4 + s_1) + 2s_4 (s_2 + s_3)}\\
      &= -J\brak{ 2(s_4 + s_1)(s_2 + s_3)} \\
      &= -2J\brak{(s_4 + s_1)(s_2+s_3)}
\end{align*}

To calculate the expectation values we need to know the possible energies and
magnetizations. The script analytic.py \footnote{\url{https://github.com/Ovewh/Computilus/tree/master/Project4}}  generates all the
possible states and lists the values we need as shown in \cref{tab:analytic}.

\begin{table}[htp]
  \centering
  \csvautotabular{../data/analytic.csv}
  \caption{Analytical values for a 2x2 grid.}
  \label{tab:analytic}
\end{table}

The probability of a given microstate is given by \cref{eq:prob}, with
the z being a normalization factor to ensure the sum of probabilities are one,
known as the partition function, given in \cref{eq:partition}.


\begin{equation}
  \label{eq:prob}
  P_i = \frac{\exp(-\beta E_i)}{z}
\end{equation}

\begin{equation}
  \label{eq:partition}
  z = \sum_{i}^{N} \exp(-\beta E_i) = \exp(8\beta) + 12 + 2\exp(-8\beta)
\end{equation}


\section{Experimental Setup}
We will
use quadratic grids with grid width L and periodic
boundary conditions.


\subsection{Equilibrium}

When we do the production runs we want to start calculating expectation values
when the system has reached equilibrium, or the most likely state. A crude
method for estimating this delay value (\# of MC cycles) is to plot energy and
magnetization against MC cycles. We did this for T $\in \brak{1.0,2.4}$ for both
ordered (all spins pointing up) and unordered initial states. From
\cref{fig:equi_E} and \cref{fig:equi_M} it is clear that the system takes longer
to reach the most likely state when the initial state is unordered. The
magnetisation seems to take longer to stabilize than the energy. At around 7500
cycles all values seem to be stable, showing small oscillations. Since we will
be running several hundred thousands MC cycles we played it safe and set the
delay parameter to 50000 in the production runs.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/equilibrium_E.png}
  \caption{Absolute values of energy for different MC cycles.
  ord refers to the initial microstate of spins. 1=up, 0=random.}
  \label{fig:equi_E}
\end{figure}


\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/equilibrium_Mabs.png}
  \caption{Absolute values of magnetization for different MC cycles.
  ord refers to the initial microstate of spins. 1=up, 0=random.}
  \label{fig:equi_M}
\end{figure}




\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{../figures/accepted.png}
  \caption{Accepted microstates scaled with number of spins and MC cycles.}
  \label{fig:accepted}
\end{figure}

\subsection{Model testing and error analysis}

Implementation of tests (test\_analytic.py, test\_ising.py) showed that the
analytical solutions was correct down to an absolute error of \num{1e-12}  when
comparing against a brute force method where we created all the possible
microstates.

Change this section to say something about the new error plots \cref{fig:error_L2}

Testing the ising model against the analytical solutions for a 2x2 grid showed
that we could consistently achieve a relative error of 0.001 with only 100000
Monte Carlo cycles (MCC) for the energy and magnetization. For the heat capacity
and susceptibility we had to increase the MCC to 5 millions and the relative
error tolerance to 0.05.

\begin{figure}[htp]
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{../figures/relative_error_order0_T1.png}
    \caption{}
    \label{fig:sub-first}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{../figures/relative_error_order1_T1.png}
    \caption{}
    \label{fig:sub-second}
  \end{subfigure}
  \hfill
  \newline
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{../figures/relative_error_order0_T2.png}
    \caption{}
    \label{fig:sub-second}
  \end{subfigure}
  \begin{subfigure}[t]{.5\textwidth}
    \centering
    \includegraphics[width=\linewidth]{../figures/relative_error_order1_T2.png}
    \caption{}
    \label{fig:sub-second}
  \end{subfigure}
\caption{Relative error vs MC cycles for L=2 for two different temperatures. Ordered
refers to the inital spin configuration, with 1 meaning all spins pointing up and
0 meaning a random configuration. Plots are smoothed by taking a
rolling mean with window size 5. The relative error decreases roughly as
$\sqrt{MC cycles}$}
\label{fig:error_L2}
\end{figure}




\subsection{Production runs}

For the production runs (phase\_transitions.py) we parallelized the code with
numba using $@njit(parallel=True)$. We found it easiest to run calculations for
each temperature in parallel, instead of parallelizing the ising model itself.
An unfortunate side effect of this was that we were not able to write to file
after each run of the ising model, so if our run did not finish we would lose
all the results. We also added a script (timings.py) that estimates the time to
run phase\_transitions.py with the given parameters so we did not start too time
consuming runs.

Timings with and without parallelization for small grid sizes showed a speedup
of roughly 3.4 on a laptop with 4 physical cores. The suboptimal speedup might
be related to creation of and writing to the 2d arrays for storing results,
but we did not look deeper into this.
